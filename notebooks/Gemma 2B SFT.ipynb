{
  "cells": [
    {
      "cell_type": "code",
      "id": "8viCYtqsbAJIKvmzdeHOnZ32",
      "metadata": {
        "tags": [],
        "id": "8viCYtqsbAJIKvmzdeHOnZ32"
      },
      "source": [
        "!pip install transformers==4.39.0 peft accelerate datasets==2.16.1 trl bitsandbytes google-cloud-secret-manager"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "from typing import Any, Optional\n",
        "\n",
        "import torch\n",
        "import wandb\n",
        "from google.cloud import secretmanager\n",
        "from datasets import Dataset, load_dataset\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, DataCollatorForLanguageModeling"
      ],
      "metadata": {
        "id": "7Ru1exxxkCfv"
      },
      "id": "7Ru1exxxkCfv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "secret_client = secretmanager.SecretManagerServiceClient()\n",
        "secret_name = f\"projects/fast-campus-machine-learning/secrets/vertex-ai-notebook/versions/1\"\n",
        "response = secret_client.access_secret_version(request={\"name\": secret_name})\n",
        "huggingface_token = response.payload.data.decode(\"UTF-8\")"
      ],
      "metadata": {
        "id": "jK_e3FMskGwG"
      },
      "id": "jK_e3FMskGwG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "I7ODE6F9kI6n"
      },
      "id": "I7ODE6F9kI6n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"google/gemma-2b-it\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=huggingface_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\", token=huggingface_token)"
      ],
      "metadata": {
        "id": "GCpf1vPAkQA1"
      },
      "id": "GCpf1vPAkQA1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuning_template = \"<start_of_turn>user\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n{response}</end_of_turn>\"\n",
        "query_template = \"<start_of_turn>user\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n\""
      ],
      "metadata": {
        "id": "zzDMeRqNkOHM"
      },
      "id": "zzDMeRqNkOHM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(instruction: str, max_new_tokens=20, device: str=\"cuda\") -> str:\n",
        "    text = query_template.format(\n",
        "        instruction=instruction\n",
        "    )\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    pattern = r\"<start_of_turn>model\\s(.+?)(<end_of_turn>|$)\"\n",
        "    match = re.search(pattern, output, re.DOTALL)\n",
        "\n",
        "    if match: return match.group(1).strip()\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "7TCKQ9VykSgr"
      },
      "id": "7TCKQ9VykSgr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Korean (Before Fine-tuned)\n",
        "print(get_response(\"Fast Campus에 대해 어떻게 생각해?\", max_new_tokens=120))"
      ],
      "metadata": {
        "id": "u6zvpMnfkTjQ"
      },
      "id": "u6zvpMnfkTjQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "usr_pattern = re.compile(r\"<usr>\\s(.+?)\\s<bot>\")\n",
        "bot_pattern = re.compile(r\"<bot>\\s(.+)\")\n",
        "\n",
        "\n",
        "def process(sample: dict[str, list[str]]) -> dict[str, list[Any]]:\n",
        "    return {\n",
        "        \"instruction\": [usr_pattern.search(t).group(1) if usr_pattern.search(t) else None for t in sample[\"text\"]],\n",
        "        \"response\": [bot_pattern.search(t).group(1) if bot_pattern.search(t) else None for t in sample[\"text\"]],\n",
        "    }\n",
        "\n",
        "\n",
        "data: Dataset = load_dataset(\"heegyu/open-korean-instructions\")\n",
        "data = data.map(process, batched=True)"
      ],
      "metadata": {
        "id": "wGY_-fAskVUz"
      },
      "id": "wGY_-fAskVUz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(example) -> list[str]:\n",
        "    return [\n",
        "        fine_tuning_template.format(\n",
        "            instruction=example[\"instruction\"][i],\n",
        "            response=example[\"response\"][i]\n",
        "        )\n",
        "        for i in range(len(example[\"instruction\"]))\n",
        "    ]\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=TrainingArguments(\n",
        "        report_to=\"none\",\n",
        "        per_device_train_batch_size=2,\n",
        "        num_train_epochs=1,\n",
        "        max_steps=500,\n",
        "        optim=\"adafactor\",\n",
        "        warmup_steps=2,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=100,\n",
        "        output_dir=\"outputs\",\n",
        "        dataloader_drop_last=True,\n",
        "    ),\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    peft_config=lora_config,\n",
        "    formatting_func=formatting_func,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Mw_aPzIAlKX7"
      },
      "id": "Mw_aPzIAlKX7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Korean\n",
        "print(get_response(\"Fast Campus에 대해 어떻게 생각해?\", max_new_tokens=120))"
      ],
      "metadata": {
        "id": "1n-DhIgOlOB0"
      },
      "id": "1n-DhIgOlOB0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sZlRyOULpS2M"
      },
      "id": "sZlRyOULpS2M",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "sniper45han (2024. 4. 1. 오전 1:47:46)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
