
==> Í∞êÏÇ¨ <==
|---------|--------------------------------|----------|--------|---------|---------------------|---------------------|
| Command |              Args              | Profile  |  User  | Version |     Start Time      |      End Time       |
|---------|--------------------------------|----------|--------|---------|---------------------|---------------------|
| start   |                                | minikube | gopher | v1.33.0 | 13 May 24 05:48 PDT |                     |
| start   |                                | minikube | gopher | v1.33.0 | 13 May 24 05:48 PDT | 13 May 24 05:55 PDT |
| service | hello-minikube                 | minikube | gopher | v1.33.0 | 13 May 24 05:55 PDT |                     |
| service | hello-minikube                 | minikube | gopher | v1.33.0 | 13 May 24 05:56 PDT |                     |
| service | kubernetes                     | minikube | gopher | v1.33.0 | 13 May 24 06:14 PDT | 13 May 24 06:14 PDT |
| service | fast-api-service -n            | minikube | gopher | v1.33.0 | 13 May 24 06:15 PDT |                     |
|         | fast-api-example               |          |        |         |                     |                     |
| service | -n fast-api-example            | minikube | gopher | v1.33.0 | 13 May 24 06:16 PDT |                     |
|---------|--------------------------------|----------|--------|---------|---------------------|---------------------|


==> ÎßàÏßÄÎßâ ÏãúÏûë <==
Log file created at: 2024/05/13 05:48:21
Running on machine: Gopherui-MacBookPro
Binary: Built with gc go1.22.2 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0513 05:48:21.299486   27816 out.go:291] Setting OutFile to fd 1 ...
I0513 05:48:21.300322   27816 out.go:343] isatty.IsTerminal(1) = true
I0513 05:48:21.300324   27816 out.go:304] Setting ErrFile to fd 2...
I0513 05:48:21.300327   27816 out.go:343] isatty.IsTerminal(2) = true
I0513 05:48:21.300488   27816 root.go:338] Updating PATH: /Users/gopher/.minikube/bin
W0513 05:48:21.300972   27816 root.go:314] Error reading config file at /Users/gopher/.minikube/config/config.json: open /Users/gopher/.minikube/config/config.json: no such file or directory
I0513 05:48:21.301837   27816 out.go:298] Setting JSON to false
I0513 05:48:21.322038   27816 start.go:129] hostinfo: {"hostname":"Gopherui-MacBookPro.local","uptime":444493,"bootTime":1715160008,"procs":655,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"13.3.1","kernelVersion":"22.4.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"e707c433-a0a6-5973-b457-47aa35fc58ed"}
W0513 05:48:21.322125   27816 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0513 05:48:21.326330   27816 out.go:177] üòÑ  Darwin 13.3.1 (arm64) Ïùò minikube v1.33.0
W0513 05:48:21.335741   27816 preload.go:294] Failed to list preload files: open /Users/gopher/.minikube/cache/preloaded-tarball: no such file or directory
I0513 05:48:21.336039   27816 notify.go:220] Checking for updates...
I0513 05:48:21.336310   27816 driver.go:392] Setting default libvirt URI to qemu:///system
I0513 05:48:21.336667   27816 global.go:112] Querying for installed drivers using PATH=/Users/gopher/.minikube/bin:/Users/gopher/miniconda3/bin:/Users/gopher/miniconda3/condabin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/go/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/gopher/Library/Application Support/JetBrains/Toolbox/scripts:/Users/gopher/miniconda3/bin:/opt/homebrew/bin:/Users/gopher/.fzf/bin
I0513 05:48:21.337268   27816 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0513 05:48:21.337289   27816 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0513 05:48:21.337487   27816 global.go:133] hyperkit default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "hyperkit": executable file not found in $PATH Reason: Fix:Run 'brew install hyperkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/hyperkit/ Version:}
I0513 05:48:21.337550   27816 global.go:133] parallels default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "prlctl": executable file not found in $PATH Reason: Fix:Install Parallels Desktop for Mac Doc:https://minikube.sigs.k8s.io/docs/drivers/parallels/ Version:}
I0513 05:48:21.337983   27816 global.go:133] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-aarch64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0513 05:48:21.338076   27816 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0513 05:48:21.338122   27816 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0513 05:48:21.388623   27816 docker.go:122] docker version: linux-24.0.7:Docker Desktop 4.26.1 (131620)
I0513 05:48:21.389064   27816 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0513 05:48:21.677690   27816 info.go:266] docker info: {ID:7b214564-4dc4-4862-a6f5-789c16ff7154 Containers:8 ContainersRunning:6 ContainersPaused:0 ContainersStopped:2 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:57 OomKillDisable:false NGoroutines:188 SystemTime:2024-05-13 12:48:21.670300668 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:6.5.11-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8227332096 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:qrizbprxjjbibjdyzs76edtcr NodeAddr:192.168.65.3 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.65.3:2377 NodeID:qrizbprxjjbibjdyzs76edtcr]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f Expected:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f} RuncCommit:{ID:v1.1.10-0-g18a0cb0 Expected:v1.1.10-0-g18a0cb0} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/gopher/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.0-desktop.2] map[Name:compose Path:/Users/gopher/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.3-desktop.2] map[Name:dev Path:/Users/gopher/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/gopher/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:/Users/gopher/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:0.1] map[Name:init Path:/Users/gopher/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.10] map[Name:sbom Path:/Users/gopher/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/gopher/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/gopher/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.2.0]] Warnings:<nil>}}
I0513 05:48:21.677781   27816 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0513 05:48:21.677790   27816 driver.go:314] not recommending "ssh" due to default: false
I0513 05:48:21.677803   27816 driver.go:349] Picked: docker
I0513 05:48:21.677806   27816 driver.go:350] Alternatives: [ssh]
I0513 05:48:21.677807   27816 driver.go:351] Rejects: [podman hyperkit parallels qemu2 virtualbox vmware]
I0513 05:48:21.684328   27816 out.go:177] ‚ú®  ÏûêÎèôÏ†ÅÏúºÎ°ú docker ÎìúÎùºÏù¥Î≤ÑÍ∞Ä ÏÑ†ÌÉùÎêòÏóàÏäµÎãàÎã§
I0513 05:48:21.687540   27816 start.go:297] selected driver: docker
I0513 05:48:21.687543   27816 start.go:901] validating driver "docker" against <nil>
I0513 05:48:21.687549   27816 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0513 05:48:21.687743   27816 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0513 05:48:21.772518   27816 info.go:266] docker info: {ID:7b214564-4dc4-4862-a6f5-789c16ff7154 Containers:8 ContainersRunning:6 ContainersPaused:0 ContainersStopped:2 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:57 OomKillDisable:false NGoroutines:188 SystemTime:2024-05-13 12:48:21.758904668 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:6.5.11-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8227332096 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:qrizbprxjjbibjdyzs76edtcr NodeAddr:192.168.65.3 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.65.3:2377 NodeID:qrizbprxjjbibjdyzs76edtcr]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f Expected:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f} RuncCommit:{ID:v1.1.10-0-g18a0cb0 Expected:v1.1.10-0-g18a0cb0} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/gopher/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.0-desktop.2] map[Name:compose Path:/Users/gopher/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.3-desktop.2] map[Name:dev Path:/Users/gopher/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/gopher/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:/Users/gopher/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:0.1] map[Name:init Path:/Users/gopher/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.10] map[Name:sbom Path:/Users/gopher/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/gopher/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/gopher/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.2.0]] Warnings:<nil>}}
I0513 05:48:21.772677   27816 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0513 05:48:21.772995   27816 start_flags.go:393] Using suggested 7798MB memory alloc based on sys=32768MB, container=7846MB
I0513 05:48:21.773395   27816 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0513 05:48:21.776329   27816 out.go:177] üìå  Using Docker Desktop driver with root privileges
I0513 05:48:21.779706   27816 cni.go:84] Creating CNI manager for ""
I0513 05:48:21.779883   27816 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0513 05:48:21.779886   27816 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0513 05:48:21.780078   27816 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:7798 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0513 05:48:21.781375   27816 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0513 05:48:21.787727   27816 cache.go:121] Beginning downloading kic base image for docker with docker
I0513 05:48:21.790296   27816 out.go:177] üöú  Pulling base image v0.0.43 ...
I0513 05:48:21.792403   27816 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0513 05:48:21.792420   27816 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon
I0513 05:48:21.836429   27816 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 to local cache
I0513 05:48:21.836884   27816 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local cache directory
I0513 05:48:21.837047   27816 image.go:118] Writing gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 to local cache
I0513 05:48:21.876585   27816 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4
I0513 05:48:21.876595   27816 cache.go:56] Caching tarball of preloaded images
I0513 05:48:21.876746   27816 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0513 05:48:21.879279   27816 out.go:177] üíæ  Ïø†Î≤ÑÎÑ§Ìã∞Ïä§ v1.30.0 ÏùÑ Îã§Ïö¥Î°úÎìú Ï§ë ...
I0513 05:48:21.883285   27816 preload.go:237] getting checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4 ...
I0513 05:48:22.043150   27816 download.go:107] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4?checksum=md5:677034533668c42fec962cc52f9b3c42 -> /Users/gopher/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4
I0513 05:53:47.744306   27816 preload.go:248] saving checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4 ...
I0513 05:53:47.744481   27816 preload.go:255] verifying checksum of /Users/gopher/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4 ...
I0513 05:53:48.353006   27816 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0513 05:53:48.353538   27816 profile.go:143] Saving config to /Users/gopher/.minikube/profiles/minikube/config.json ...
I0513 05:53:48.353812   27816 lock.go:35] WriteFile acquiring /Users/gopher/.minikube/profiles/minikube/config.json: {Name:mk1f309d12276e0ce88fd9dcb9350db5a9c2133f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 05:54:38.986951   27816 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 as a tarball
I0513 05:54:38.986987   27816 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 from local cache
I0513 05:54:51.582084   27816 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 from cached tarball
I0513 05:54:51.582363   27816 cache.go:194] Successfully downloaded all kic artifacts
I0513 05:54:51.584193   27816 start.go:360] acquireMachinesLock for minikube: {Name:mke9aa40a87d16674af572ae839bb1482a05022a Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0513 05:54:51.585251   27816 start.go:364] duration metric: took 990.083¬µs to acquireMachinesLock for "minikube"
I0513 05:54:51.585368   27816 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:7798 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0513 05:54:51.585711   27816 start.go:125] createHost starting for "" (driver="docker")
I0513 05:54:51.606369   27816 out.go:204] üî•  Creating docker container (CPUs=2, Memory=7798MB) ...
I0513 05:54:51.607228   27816 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0513 05:54:51.607976   27816 client.go:168] LocalClient.Create starting
I0513 05:54:51.609206   27816 main.go:141] libmachine: Creating CA: /Users/gopher/.minikube/certs/ca.pem
I0513 05:54:51.672437   27816 main.go:141] libmachine: Creating client certificate: /Users/gopher/.minikube/certs/cert.pem
I0513 05:54:51.826095   27816 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0513 05:54:51.869943   27816 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0513 05:54:51.870027   27816 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0513 05:54:51.870045   27816 cli_runner.go:164] Run: docker network inspect minikube
W0513 05:54:51.906655   27816 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0513 05:54:51.906708   27816 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0513 05:54:51.906719   27816 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0513 05:54:51.906852   27816 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0513 05:54:51.952838   27816 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0x14002c65190}
I0513 05:54:51.956012   27816 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 65535 ...
I0513 05:54:51.956108   27816 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
W0513 05:54:51.993947   27816 cli_runner.go:211] docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube returned with exit code 1
W0513 05:54:51.993973   27816 network_create.go:149] failed to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and mtu of 65535: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube: exit status 1
stdout:

stderr:
Error response from daemon: Pool overlaps with other one on this address space
W0513 05:54:51.994262   27816 network_create.go:116] failed to create docker network minikube 192.168.49.0/24, will retry: subnet is taken
I0513 05:54:51.995677   27816 network.go:209] skipping subnet 192.168.49.0/24 that is reserved: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:<nil>}
I0513 05:54:51.996023   27816 network.go:206] using free private subnet 192.168.58.0/24: &{IP:192.168.58.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.58.0/24 Gateway:192.168.58.1 ClientMin:192.168.58.2 ClientMax:192.168.58.254 Broadcast:192.168.58.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0x14002453600}
I0513 05:54:51.996029   27816 network_create.go:124] attempt to create docker network minikube 192.168.58.0/24 with gateway 192.168.58.1 and MTU of 65535 ...
I0513 05:54:51.996077   27816 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.58.0/24 --gateway=192.168.58.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
W0513 05:54:52.033746   27816 cli_runner.go:211] docker network create --driver=bridge --subnet=192.168.58.0/24 --gateway=192.168.58.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube returned with exit code 1
W0513 05:54:52.033769   27816 network_create.go:149] failed to create docker network minikube 192.168.58.0/24 with gateway 192.168.58.1 and mtu of 65535: docker network create --driver=bridge --subnet=192.168.58.0/24 --gateway=192.168.58.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube: exit status 1
stdout:

stderr:
Error response from daemon: Pool overlaps with other one on this address space
W0513 05:54:52.033782   27816 network_create.go:116] failed to create docker network minikube 192.168.58.0/24, will retry: subnet is taken
I0513 05:54:52.035166   27816 network.go:209] skipping subnet 192.168.58.0/24 that is reserved: &{IP:192.168.58.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.58.0/24 Gateway:192.168.58.1 ClientMin:192.168.58.2 ClientMax:192.168.58.254 Broadcast:192.168.58.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:<nil>}
I0513 05:54:52.035540   27816 network.go:206] using free private subnet 192.168.67.0/24: &{IP:192.168.67.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.67.0/24 Gateway:192.168.67.1 ClientMin:192.168.67.2 ClientMax:192.168.67.254 Broadcast:192.168.67.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0x14002c7bba0}
I0513 05:54:52.035548   27816 network_create.go:124] attempt to create docker network minikube 192.168.67.0/24 with gateway 192.168.67.1 and MTU of 65535 ...
I0513 05:54:52.035615   27816 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.67.0/24 --gateway=192.168.67.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0513 05:54:52.091771   27816 network_create.go:108] docker network minikube 192.168.67.0/24 created
I0513 05:54:52.091831   27816 kic.go:121] calculated static IP "192.168.67.2" for the "minikube" container
I0513 05:54:52.091947   27816 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0513 05:54:52.132675   27816 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0513 05:54:52.171231   27816 oci.go:103] Successfully created a docker volume minikube
I0513 05:54:52.171391   27816 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib
I0513 05:54:54.320666   27816 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -d /var/lib: (2.149196917s)
I0513 05:54:54.320710   27816 oci.go:107] Successfully prepared a docker volume minikube
I0513 05:54:54.320744   27816 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0513 05:54:54.320783   27816 kic.go:194] Starting extracting preloaded images to volume ...
I0513 05:54:54.324258   27816 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/gopher/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir
I0513 05:54:57.901161   27816 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/gopher/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 -I lz4 -xf /preloaded.tar -C /extractDir: (3.5767725s)
I0513 05:54:57.901227   27816 kic.go:203] duration metric: took 3.580430958s to extract preloaded images to volume ...
I0513 05:54:57.901540   27816 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0513 05:54:58.332213   27816 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.67.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=7798mb --memory-swap=7798mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737
I0513 05:54:58.603091   27816 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0513 05:54:58.657948   27816 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0513 05:54:58.697926   27816 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0513 05:54:58.787133   27816 oci.go:144] the created container "minikube" has a running status.
I0513 05:54:58.787431   27816 kic.go:225] Creating ssh key for kic: /Users/gopher/.minikube/machines/minikube/id_rsa...
I0513 05:54:58.939281   27816 kic_runner.go:191] docker (temp): /Users/gopher/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0513 05:54:59.004909   27816 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0513 05:54:59.057055   27816 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0513 05:54:59.057097   27816 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0513 05:54:59.138465   27816 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0513 05:54:59.180361   27816 machine.go:94] provisionDockerMachine start ...
I0513 05:54:59.180496   27816 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 05:54:59.224343   27816 main.go:141] libmachine: Using SSH client type: native
I0513 05:54:59.224522   27816 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10508b280] 0x10508dae0 <nil>  [] 0s} 127.0.0.1 61537 <nil> <nil>}
I0513 05:54:59.224525   27816 main.go:141] libmachine: About to run SSH command:
hostname
I0513 05:54:59.349382   27816 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0513 05:54:59.353392   27816 ubuntu.go:169] provisioning hostname "minikube"
I0513 05:54:59.353507   27816 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 05:54:59.394725   27816 main.go:141] libmachine: Using SSH client type: native
I0513 05:54:59.394900   27816 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10508b280] 0x10508dae0 <nil>  [] 0s} 127.0.0.1 61537 <nil> <nil>}
I0513 05:54:59.394907   27816 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0513 05:54:59.534783   27816 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0513 05:54:59.534874   27816 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 05:54:59.578645   27816 main.go:141] libmachine: Using SSH client type: native
I0513 05:54:59.578819   27816 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10508b280] 0x10508dae0 <nil>  [] 0s} 127.0.0.1 61537 <nil> <nil>}
I0513 05:54:59.578825   27816 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0513 05:54:59.703498   27816 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0513 05:54:59.703544   27816 ubuntu.go:175] set auth options {CertDir:/Users/gopher/.minikube CaCertPath:/Users/gopher/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/gopher/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/gopher/.minikube/machines/server.pem ServerKeyPath:/Users/gopher/.minikube/machines/server-key.pem ClientKeyPath:/Users/gopher/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/gopher/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/gopher/.minikube}
I0513 05:54:59.703619   27816 ubuntu.go:177] setting up certificates
I0513 05:54:59.703642   27816 provision.go:84] configureAuth start
I0513 05:54:59.703991   27816 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0513 05:54:59.755445   27816 provision.go:143] copyHostCerts
I0513 05:54:59.755837   27816 exec_runner.go:151] cp: /Users/gopher/.minikube/certs/ca.pem --> /Users/gopher/.minikube/ca.pem (1078 bytes)
I0513 05:54:59.756074   27816 exec_runner.go:151] cp: /Users/gopher/.minikube/certs/cert.pem --> /Users/gopher/.minikube/cert.pem (1123 bytes)
I0513 05:54:59.756343   27816 exec_runner.go:151] cp: /Users/gopher/.minikube/certs/key.pem --> /Users/gopher/.minikube/key.pem (1675 bytes)
I0513 05:54:59.756582   27816 provision.go:117] generating server cert: /Users/gopher/.minikube/machines/server.pem ca-key=/Users/gopher/.minikube/certs/ca.pem private-key=/Users/gopher/.minikube/certs/ca-key.pem org=gopher.minikube san=[127.0.0.1 192.168.67.2 localhost minikube]
I0513 05:54:59.863565   27816 provision.go:177] copyRemoteCerts
I0513 05:54:59.864120   27816 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0513 05:54:59.864171   27816 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 05:54:59.901116   27816 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61537 SSHKeyPath:/Users/gopher/.minikube/machines/minikube/id_rsa Username:docker}
I0513 05:54:59.987568   27816 ssh_runner.go:362] scp /Users/gopher/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0513 05:54:59.998484   27816 ssh_runner.go:362] scp /Users/gopher/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I0513 05:55:00.009859   27816 ssh_runner.go:362] scp /Users/gopher/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0513 05:55:00.022243   27816 provision.go:87] duration metric: took 318.590209ms to configureAuth
I0513 05:55:00.022253   27816 ubuntu.go:193] setting minikube options for container-runtime
I0513 05:55:00.025819   27816 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0513 05:55:00.025886   27816 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 05:55:00.067446   27816 main.go:141] libmachine: Using SSH client type: native
I0513 05:55:00.067632   27816 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10508b280] 0x10508dae0 <nil>  [] 0s} 127.0.0.1 61537 <nil> <nil>}
I0513 05:55:00.067637   27816 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0513 05:55:00.193224   27816 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0513 05:55:00.193232   27816 ubuntu.go:71] root file system type: overlay
I0513 05:55:00.193301   27816 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0513 05:55:00.193389   27816 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 05:55:00.240702   27816 main.go:141] libmachine: Using SSH client type: native
I0513 05:55:00.240889   27816 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10508b280] 0x10508dae0 <nil>  [] 0s} 127.0.0.1 61537 <nil> <nil>}
I0513 05:55:00.240923   27816 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0513 05:55:00.369067   27816 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0513 05:55:00.369308   27816 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 05:55:00.420419   27816 main.go:141] libmachine: Using SSH client type: native
I0513 05:55:00.420602   27816 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10508b280] 0x10508dae0 <nil>  [] 0s} 127.0.0.1 61537 <nil> <nil>}
I0513 05:55:00.420610   27816 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0513 05:55:00.883890   27816 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-11 10:51:51.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-05-13 12:55:00.367160009 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0513 05:55:00.883922   27816 machine.go:97] duration metric: took 1.703535625s to provisionDockerMachine
I0513 05:55:00.883936   27816 client.go:171] duration metric: took 9.275937417s to LocalClient.Create
I0513 05:55:00.883961   27816 start.go:167] duration metric: took 9.27672425s to libmachine.API.Create "minikube"
I0513 05:55:00.883969   27816 start.go:293] postStartSetup for "minikube" (driver="docker")
I0513 05:55:00.883980   27816 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0513 05:55:00.884236   27816 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0513 05:55:00.884327   27816 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 05:55:00.962194   27816 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61537 SSHKeyPath:/Users/gopher/.minikube/machines/minikube/id_rsa Username:docker}
I0513 05:55:01.055314   27816 ssh_runner.go:195] Run: cat /etc/os-release
I0513 05:55:01.057560   27816 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0513 05:55:01.057594   27816 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0513 05:55:01.057598   27816 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0513 05:55:01.057601   27816 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0513 05:55:01.058176   27816 filesync.go:126] Scanning /Users/gopher/.minikube/addons for local assets ...
I0513 05:55:01.058263   27816 filesync.go:126] Scanning /Users/gopher/.minikube/files for local assets ...
I0513 05:55:01.058286   27816 start.go:296] duration metric: took 174.314167ms for postStartSetup
I0513 05:55:01.071869   27816 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0513 05:55:01.116656   27816 profile.go:143] Saving config to /Users/gopher/.minikube/profiles/minikube/config.json ...
I0513 05:55:01.122677   27816 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0513 05:55:01.122718   27816 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 05:55:01.176813   27816 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61537 SSHKeyPath:/Users/gopher/.minikube/machines/minikube/id_rsa Username:docker}
I0513 05:55:01.265758   27816 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0513 05:55:01.268667   27816 start.go:128] duration metric: took 9.682802541s to createHost
I0513 05:55:01.268684   27816 start.go:83] releasing machines lock for "minikube", held for 9.683397792s
I0513 05:55:01.268780   27816 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0513 05:55:01.330684   27816 ssh_runner.go:195] Run: cat /version.json
I0513 05:55:01.330736   27816 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 05:55:01.331153   27816 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0513 05:55:01.331462   27816 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 05:55:01.389227   27816 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61537 SSHKeyPath:/Users/gopher/.minikube/machines/minikube/id_rsa Username:docker}
I0513 05:55:01.394888   27816 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61537 SSHKeyPath:/Users/gopher/.minikube/machines/minikube/id_rsa Username:docker}
I0513 05:55:01.528459   27816 ssh_runner.go:195] Run: systemctl --version
I0513 05:55:01.672590   27816 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0513 05:55:01.675168   27816 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0513 05:55:01.686433   27816 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0513 05:55:01.686482   27816 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0513 05:55:01.698304   27816 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0513 05:55:01.698319   27816 start.go:494] detecting cgroup driver to use...
I0513 05:55:01.698335   27816 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0513 05:55:01.699168   27816 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0513 05:55:01.706454   27816 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0513 05:55:01.712088   27816 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0513 05:55:01.716269   27816 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0513 05:55:01.716361   27816 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0513 05:55:01.720914   27816 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0513 05:55:01.725548   27816 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0513 05:55:01.730798   27816 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0513 05:55:01.735304   27816 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0513 05:55:01.739489   27816 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0513 05:55:01.744844   27816 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0513 05:55:01.749636   27816 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0513 05:55:01.754243   27816 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0513 05:55:01.757967   27816 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0513 05:55:01.761736   27816 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0513 05:55:01.795142   27816 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0513 05:55:01.844355   27816 start.go:494] detecting cgroup driver to use...
I0513 05:55:01.844408   27816 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0513 05:55:01.844548   27816 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0513 05:55:01.850983   27816 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0513 05:55:01.851100   27816 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0513 05:55:01.857808   27816 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0513 05:55:01.867973   27816 ssh_runner.go:195] Run: which cri-dockerd
I0513 05:55:01.870042   27816 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0513 05:55:01.874743   27816 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0513 05:55:01.884600   27816 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0513 05:55:01.920037   27816 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0513 05:55:01.957149   27816 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0513 05:55:01.957291   27816 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0513 05:55:01.966270   27816 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0513 05:55:02.000598   27816 ssh_runner.go:195] Run: sudo systemctl restart docker
I0513 05:55:02.200453   27816 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0513 05:55:02.207252   27816 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0513 05:55:02.213404   27816 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0513 05:55:02.249177   27816 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0513 05:55:02.283083   27816 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0513 05:55:02.316360   27816 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0513 05:55:02.331975   27816 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0513 05:55:02.338057   27816 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0513 05:55:02.371605   27816 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0513 05:55:02.413729   27816 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0513 05:55:02.418417   27816 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0513 05:55:02.420859   27816 start.go:562] Will wait 60s for crictl version
I0513 05:55:02.420940   27816 ssh_runner.go:195] Run: which crictl
I0513 05:55:02.422992   27816 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0513 05:55:02.441443   27816 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0513 05:55:02.441552   27816 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0513 05:55:02.453920   27816 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0513 05:55:02.467252   27816 out.go:204] üê≥  Ïø†Î≤ÑÎÑ§Ìã∞Ïä§ v1.30.0 ÏùÑ Docker 26.0.1 Îü∞ÌÉÄÏûÑÏúºÎ°ú ÏÑ§ÏπòÌïòÎäî Ï§ë
I0513 05:55:02.467418   27816 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0513 05:55:02.568525   27816 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0513 05:55:02.568899   27816 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0513 05:55:02.571282   27816 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0513 05:55:02.576958   27816 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0513 05:55:02.615160   27816 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:7798 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0513 05:55:02.615237   27816 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0513 05:55:02.615294   27816 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0513 05:55:02.624539   27816 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0513 05:55:02.624546   27816 docker.go:615] Images already preloaded, skipping extraction
I0513 05:55:02.624848   27816 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0513 05:55:02.632251   27816 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0513 05:55:02.632262   27816 cache_images.go:84] Images are preloaded, skipping loading
I0513 05:55:02.632266   27816 kubeadm.go:928] updating node { 192.168.67.2 8443 v1.30.0 docker true true} ...
I0513 05:55:02.632574   27816 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.67.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0513 05:55:02.632614   27816 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0513 05:55:02.656333   27816 cni.go:84] Creating CNI manager for ""
I0513 05:55:02.656344   27816 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0513 05:55:02.656351   27816 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0513 05:55:02.656363   27816 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.67.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.67.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.67.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0513 05:55:02.656479   27816 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.67.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.67.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.67.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0513 05:55:02.656557   27816 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0513 05:55:02.661030   27816 binaries.go:44] Found k8s binaries, skipping transfer
I0513 05:55:02.661089   27816 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0513 05:55:02.665142   27816 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0513 05:55:02.673683   27816 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0513 05:55:02.681907   27816 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0513 05:55:02.690262   27816 ssh_runner.go:195] Run: grep 192.168.67.2	control-plane.minikube.internal$ /etc/hosts
I0513 05:55:02.692246   27816 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.67.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0513 05:55:02.697496   27816 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0513 05:55:02.733028   27816 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0513 05:55:02.759044   27816 certs.go:68] Setting up /Users/gopher/.minikube/profiles/minikube for IP: 192.168.67.2
I0513 05:55:02.759062   27816 certs.go:194] generating shared ca certs ...
I0513 05:55:02.759087   27816 certs.go:226] acquiring lock for ca certs: {Name:mkb1e9c58c68bdee3321fd68995937f03836ccfa Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 05:55:02.759621   27816 certs.go:240] generating "minikubeCA" ca cert: /Users/gopher/.minikube/ca.key
I0513 05:55:02.958139   27816 crypto.go:156] Writing cert to /Users/gopher/.minikube/ca.crt ...
I0513 05:55:02.958152   27816 lock.go:35] WriteFile acquiring /Users/gopher/.minikube/ca.crt: {Name:mk689ad637435b1df884b1684f5c45149f731c6f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 05:55:02.958474   27816 crypto.go:164] Writing key to /Users/gopher/.minikube/ca.key ...
I0513 05:55:02.958477   27816 lock.go:35] WriteFile acquiring /Users/gopher/.minikube/ca.key: {Name:mkb9619bd45120566cd0edfbd60cd108a7a7a21a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 05:55:02.958632   27816 certs.go:240] generating "proxyClientCA" ca cert: /Users/gopher/.minikube/proxy-client-ca.key
I0513 05:55:03.099486   27816 crypto.go:156] Writing cert to /Users/gopher/.minikube/proxy-client-ca.crt ...
I0513 05:55:03.099496   27816 lock.go:35] WriteFile acquiring /Users/gopher/.minikube/proxy-client-ca.crt: {Name:mk7205e9083163966e52fba4f745db0ba8b01835 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 05:55:03.099812   27816 crypto.go:164] Writing key to /Users/gopher/.minikube/proxy-client-ca.key ...
I0513 05:55:03.099816   27816 lock.go:35] WriteFile acquiring /Users/gopher/.minikube/proxy-client-ca.key: {Name:mk9d54019fb9ad489d6e637b0189a3a2a7ed670e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 05:55:03.099975   27816 certs.go:256] generating profile certs ...
I0513 05:55:03.100012   27816 certs.go:363] generating signed profile cert for "minikube-user": /Users/gopher/.minikube/profiles/minikube/client.key
I0513 05:55:03.100304   27816 crypto.go:68] Generating cert /Users/gopher/.minikube/profiles/minikube/client.crt with IP's: []
I0513 05:55:03.159766   27816 crypto.go:156] Writing cert to /Users/gopher/.minikube/profiles/minikube/client.crt ...
I0513 05:55:03.159774   27816 lock.go:35] WriteFile acquiring /Users/gopher/.minikube/profiles/minikube/client.crt: {Name:mkb647ed719899249d05144d35baeac5f2a0e91c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 05:55:03.160033   27816 crypto.go:164] Writing key to /Users/gopher/.minikube/profiles/minikube/client.key ...
I0513 05:55:03.160035   27816 lock.go:35] WriteFile acquiring /Users/gopher/.minikube/profiles/minikube/client.key: {Name:mke88717e29fe16ca8e0651ac1c354d421a07c04 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 05:55:03.160163   27816 certs.go:363] generating signed profile cert for "minikube": /Users/gopher/.minikube/profiles/minikube/apiserver.key.583c145e
I0513 05:55:03.160175   27816 crypto.go:68] Generating cert /Users/gopher/.minikube/profiles/minikube/apiserver.crt.583c145e with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.67.2]
I0513 05:55:03.285668   27816 crypto.go:156] Writing cert to /Users/gopher/.minikube/profiles/minikube/apiserver.crt.583c145e ...
I0513 05:55:03.285681   27816 lock.go:35] WriteFile acquiring /Users/gopher/.minikube/profiles/minikube/apiserver.crt.583c145e: {Name:mk3f01cfa109e5cc859dd49d405b797225bc58c8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 05:55:03.286030   27816 crypto.go:164] Writing key to /Users/gopher/.minikube/profiles/minikube/apiserver.key.583c145e ...
I0513 05:55:03.286033   27816 lock.go:35] WriteFile acquiring /Users/gopher/.minikube/profiles/minikube/apiserver.key.583c145e: {Name:mkc9e2f9923bf90f9a989edef7035e8312423033 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 05:55:03.286226   27816 certs.go:381] copying /Users/gopher/.minikube/profiles/minikube/apiserver.crt.583c145e -> /Users/gopher/.minikube/profiles/minikube/apiserver.crt
I0513 05:55:03.286359   27816 certs.go:385] copying /Users/gopher/.minikube/profiles/minikube/apiserver.key.583c145e -> /Users/gopher/.minikube/profiles/minikube/apiserver.key
I0513 05:55:03.286454   27816 certs.go:363] generating signed profile cert for "aggregator": /Users/gopher/.minikube/profiles/minikube/proxy-client.key
I0513 05:55:03.286468   27816 crypto.go:68] Generating cert /Users/gopher/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0513 05:55:03.326152   27816 crypto.go:156] Writing cert to /Users/gopher/.minikube/profiles/minikube/proxy-client.crt ...
I0513 05:55:03.326160   27816 lock.go:35] WriteFile acquiring /Users/gopher/.minikube/profiles/minikube/proxy-client.crt: {Name:mkcfe2cd41017c850513668f3e9a6e11f428c0d6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 05:55:03.326460   27816 crypto.go:164] Writing key to /Users/gopher/.minikube/profiles/minikube/proxy-client.key ...
I0513 05:55:03.326464   27816 lock.go:35] WriteFile acquiring /Users/gopher/.minikube/profiles/minikube/proxy-client.key: {Name:mk28f5753e4c776293ee0e9874ce0b3f8da15fb2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 05:55:03.326865   27816 certs.go:484] found cert: /Users/gopher/.minikube/certs/ca-key.pem (1679 bytes)
I0513 05:55:03.326893   27816 certs.go:484] found cert: /Users/gopher/.minikube/certs/ca.pem (1078 bytes)
I0513 05:55:03.326926   27816 certs.go:484] found cert: /Users/gopher/.minikube/certs/cert.pem (1123 bytes)
I0513 05:55:03.327147   27816 certs.go:484] found cert: /Users/gopher/.minikube/certs/key.pem (1675 bytes)
I0513 05:55:03.329370   27816 ssh_runner.go:362] scp /Users/gopher/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0513 05:55:03.341382   27816 ssh_runner.go:362] scp /Users/gopher/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0513 05:55:03.352881   27816 ssh_runner.go:362] scp /Users/gopher/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0513 05:55:03.363970   27816 ssh_runner.go:362] scp /Users/gopher/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0513 05:55:03.375258   27816 ssh_runner.go:362] scp /Users/gopher/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0513 05:55:03.386340   27816 ssh_runner.go:362] scp /Users/gopher/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0513 05:55:03.396977   27816 ssh_runner.go:362] scp /Users/gopher/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0513 05:55:03.408272   27816 ssh_runner.go:362] scp /Users/gopher/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0513 05:55:03.419802   27816 ssh_runner.go:362] scp /Users/gopher/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0513 05:55:03.430469   27816 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0513 05:55:03.442585   27816 ssh_runner.go:195] Run: openssl version
I0513 05:55:03.446524   27816 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0513 05:55:03.452305   27816 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0513 05:55:03.454448   27816 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 13 12:55 /usr/share/ca-certificates/minikubeCA.pem
I0513 05:55:03.454498   27816 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0513 05:55:03.458034   27816 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0513 05:55:03.462368   27816 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0513 05:55:03.464127   27816 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0513 05:55:03.464172   27816 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:7798 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0513 05:55:03.464246   27816 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0513 05:55:03.472361   27816 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0513 05:55:03.477383   27816 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0513 05:55:03.482189   27816 kubeadm.go:213] ignoring SystemVerification for kubeadm because of docker driver
I0513 05:55:03.482225   27816 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0513 05:55:03.486782   27816 kubeadm.go:154] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0513 05:55:03.486785   27816 kubeadm.go:156] found existing configuration files:

I0513 05:55:03.486820   27816 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0513 05:55:03.491821   27816 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0513 05:55:03.491861   27816 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0513 05:55:03.496179   27816 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0513 05:55:03.500641   27816 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0513 05:55:03.500672   27816 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0513 05:55:03.505142   27816 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0513 05:55:03.509888   27816 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0513 05:55:03.509930   27816 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0513 05:55:03.514259   27816 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0513 05:55:03.518907   27816 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0513 05:55:03.518949   27816 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0513 05:55:03.523304   27816 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0513 05:55:03.562869   27816 kubeadm.go:309] [init] Using Kubernetes version: v1.30.0
I0513 05:55:03.562938   27816 kubeadm.go:309] [preflight] Running pre-flight checks
I0513 05:55:03.614290   27816 kubeadm.go:309] [preflight] Pulling images required for setting up a Kubernetes cluster
I0513 05:55:03.614415   27816 kubeadm.go:309] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0513 05:55:03.614529   27816 kubeadm.go:309] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0513 05:55:03.710724   27816 kubeadm.go:309] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0513 05:55:03.714817   27816 out.go:204]     ‚ñ™ Ïù∏Ï¶ùÏÑú Î∞è ÌÇ§Î•º ÏÉùÏÑ±ÌïòÎäî Ï§ë ...
I0513 05:55:03.714956   27816 kubeadm.go:309] [certs] Using existing ca certificate authority
I0513 05:55:03.715015   27816 kubeadm.go:309] [certs] Using existing apiserver certificate and key on disk
I0513 05:55:03.778788   27816 kubeadm.go:309] [certs] Generating "apiserver-kubelet-client" certificate and key
I0513 05:55:03.887922   27816 kubeadm.go:309] [certs] Generating "front-proxy-ca" certificate and key
I0513 05:55:04.096730   27816 kubeadm.go:309] [certs] Generating "front-proxy-client" certificate and key
I0513 05:55:04.146309   27816 kubeadm.go:309] [certs] Generating "etcd/ca" certificate and key
I0513 05:55:04.202369   27816 kubeadm.go:309] [certs] Generating "etcd/server" certificate and key
I0513 05:55:04.202516   27816 kubeadm.go:309] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.67.2 127.0.0.1 ::1]
I0513 05:55:04.275650   27816 kubeadm.go:309] [certs] Generating "etcd/peer" certificate and key
I0513 05:55:04.275923   27816 kubeadm.go:309] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.67.2 127.0.0.1 ::1]
I0513 05:55:04.381836   27816 kubeadm.go:309] [certs] Generating "etcd/healthcheck-client" certificate and key
I0513 05:55:04.473033   27816 kubeadm.go:309] [certs] Generating "apiserver-etcd-client" certificate and key
I0513 05:55:04.509798   27816 kubeadm.go:309] [certs] Generating "sa" key and public key
I0513 05:55:04.509895   27816 kubeadm.go:309] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0513 05:55:04.635697   27816 kubeadm.go:309] [kubeconfig] Writing "admin.conf" kubeconfig file
I0513 05:55:04.754495   27816 kubeadm.go:309] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0513 05:55:04.805439   27816 kubeadm.go:309] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0513 05:55:04.949116   27816 kubeadm.go:309] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0513 05:55:05.053949   27816 kubeadm.go:309] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0513 05:55:05.054351   27816 kubeadm.go:309] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0513 05:55:05.055452   27816 kubeadm.go:309] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0513 05:55:05.066334   27816 out.go:204]     ‚ñ™ Ïª®Ìä∏Î°§ ÌîåÎ†àÏù∏ÏùÑ Î∂ÄÌåÖÌïòÎäî Ï§ë ...
I0513 05:55:05.066485   27816 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0513 05:55:05.066569   27816 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0513 05:55:05.066637   27816 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0513 05:55:05.066771   27816 kubeadm.go:309] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0513 05:55:05.066876   27816 kubeadm.go:309] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0513 05:55:05.066915   27816 kubeadm.go:309] [kubelet-start] Starting the kubelet
I0513 05:55:05.113976   27816 kubeadm.go:309] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0513 05:55:05.114078   27816 kubeadm.go:309] [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
I0513 05:55:05.616146   27816 kubeadm.go:309] [kubelet-check] The kubelet is healthy after 501.840584ms
I0513 05:55:05.616407   27816 kubeadm.go:309] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0513 05:55:10.118290   27816 kubeadm.go:309] [api-check] The API server is healthy after 4.501872627s
I0513 05:55:10.125023   27816 kubeadm.go:309] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0513 05:55:10.129922   27816 kubeadm.go:309] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0513 05:55:10.137048   27816 kubeadm.go:309] [upload-certs] Skipping phase. Please see --upload-certs
I0513 05:55:10.137315   27816 kubeadm.go:309] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0513 05:55:10.140154   27816 kubeadm.go:309] [bootstrap-token] Using token: un0xe0.1hd18fbw8ok7mamg
I0513 05:55:10.142306   27816 out.go:204]     ‚ñ™ RBAC Í∑úÏπôÏùÑ Íµ¨ÏÑ±ÌïòÎäî Ï§ë ...
I0513 05:55:10.142483   27816 kubeadm.go:309] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0513 05:55:10.143328   27816 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0513 05:55:10.147156   27816 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0513 05:55:10.148647   27816 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0513 05:55:10.149531   27816 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0513 05:55:10.150336   27816 kubeadm.go:309] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0513 05:55:10.522431   27816 kubeadm.go:309] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0513 05:55:10.930973   27816 kubeadm.go:309] [addons] Applied essential addon: CoreDNS
I0513 05:55:11.522431   27816 kubeadm.go:309] [addons] Applied essential addon: kube-proxy
I0513 05:55:11.523003   27816 kubeadm.go:309] 
I0513 05:55:11.523092   27816 kubeadm.go:309] Your Kubernetes control-plane has initialized successfully!
I0513 05:55:11.523096   27816 kubeadm.go:309] 
I0513 05:55:11.523202   27816 kubeadm.go:309] To start using your cluster, you need to run the following as a regular user:
I0513 05:55:11.523210   27816 kubeadm.go:309] 
I0513 05:55:11.523281   27816 kubeadm.go:309]   mkdir -p $HOME/.kube
I0513 05:55:11.523358   27816 kubeadm.go:309]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0513 05:55:11.523407   27816 kubeadm.go:309]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0513 05:55:11.523410   27816 kubeadm.go:309] 
I0513 05:55:11.523488   27816 kubeadm.go:309] Alternatively, if you are the root user, you can run:
I0513 05:55:11.523492   27816 kubeadm.go:309] 
I0513 05:55:11.523544   27816 kubeadm.go:309]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0513 05:55:11.523550   27816 kubeadm.go:309] 
I0513 05:55:11.523611   27816 kubeadm.go:309] You should now deploy a pod network to the cluster.
I0513 05:55:11.523708   27816 kubeadm.go:309] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0513 05:55:11.523792   27816 kubeadm.go:309]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0513 05:55:11.523798   27816 kubeadm.go:309] 
I0513 05:55:11.523884   27816 kubeadm.go:309] You can now join any number of control-plane nodes by copying certificate authorities
I0513 05:55:11.523959   27816 kubeadm.go:309] and service account keys on each node and then running the following as root:
I0513 05:55:11.523962   27816 kubeadm.go:309] 
I0513 05:55:11.524062   27816 kubeadm.go:309]   kubeadm join control-plane.minikube.internal:8443 --token un0xe0.1hd18fbw8ok7mamg \
I0513 05:55:11.524232   27816 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:0df6cba0551db23b3e489febea063bab619ce19434aa44b9389b556509c2f6b4 \
I0513 05:55:11.524267   27816 kubeadm.go:309] 	--control-plane 
I0513 05:55:11.524289   27816 kubeadm.go:309] 
I0513 05:55:11.524433   27816 kubeadm.go:309] Then you can join any number of worker nodes by running the following on each as root:
I0513 05:55:11.524495   27816 kubeadm.go:309] 
I0513 05:55:11.524638   27816 kubeadm.go:309] kubeadm join control-plane.minikube.internal:8443 --token un0xe0.1hd18fbw8ok7mamg \
I0513 05:55:11.524827   27816 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:0df6cba0551db23b3e489febea063bab619ce19434aa44b9389b556509c2f6b4 
I0513 05:55:11.525342   27816 kubeadm.go:309] 	[WARNING Swap]: swap is supported for cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default
I0513 05:55:11.525427   27816 kubeadm.go:309] 	[WARNING SystemVerification]: missing optional cgroups: hugetlb
I0513 05:55:11.525553   27816 kubeadm.go:309] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0513 05:55:11.525570   27816 cni.go:84] Creating CNI manager for ""
I0513 05:55:11.525595   27816 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0513 05:55:11.531250   27816 out.go:177] üîó  bridge CNI (Container Networking Interface) Î•º Íµ¨ÏÑ±ÌïòÎäî Ï§ë ...
I0513 05:55:11.532758   27816 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0513 05:55:11.538908   27816 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0513 05:55:11.548032   27816 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0513 05:55:11.548208   27816 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_05_13T05_55_11_0700 minikube.k8s.io/version=v1.33.0 minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0513 05:55:11.548268   27816 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0513 05:55:11.553487   27816 ops.go:34] apiserver oom_adj: -16
I0513 05:55:11.598375   27816 kubeadm.go:1107] duration metric: took 50.328542ms to wait for elevateKubeSystemPrivileges
W0513 05:55:11.604960   27816 kubeadm.go:286] apiserver tunnel failed: apiserver port not set
I0513 05:55:11.604979   27816 kubeadm.go:393] duration metric: took 8.140794209s to StartCluster
I0513 05:55:11.604996   27816 settings.go:142] acquiring lock: {Name:mkd5e8eac2ccd8ad9865027d469d9a918bad4975 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 05:55:11.605337   27816 settings.go:150] Updating kubeconfig:  /Users/gopher/.kube/config
I0513 05:55:11.611944   27816 lock.go:35] WriteFile acquiring /Users/gopher/.kube/config: {Name:mk0113a7d9c8112b9b667fe16950b58f25b063a0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 05:55:11.613858   27816 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0513 05:55:11.614079   27816 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0513 05:55:11.617309   27816 out.go:177] üîé  Kubernetes Íµ¨ÏÑ± ÏöîÏÜåÎ•º ÌôïÏù∏...
I0513 05:55:11.614284   27816 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0513 05:55:11.614340   27816 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0513 05:55:11.617365   27816 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0513 05:55:11.617369   27816 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0513 05:55:11.623663   27816 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0513 05:55:11.623675   27816 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0513 05:55:11.623712   27816 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0513 05:55:11.623876   27816 host.go:66] Checking if "minikube" exists ...
I0513 05:55:11.624644   27816 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0513 05:55:11.624771   27816 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0513 05:55:11.651935   27816 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0513 05:55:11.686566   27816 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0513 05:55:11.687829   27816 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0513 05:55:11.689587   27816 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0513 05:55:11.689593   27816 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0513 05:55:11.689640   27816 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 05:55:11.696931   27816 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0513 05:55:11.696970   27816 host.go:66] Checking if "minikube" exists ...
I0513 05:55:11.697231   27816 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0513 05:55:11.733602   27816 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61537 SSHKeyPath:/Users/gopher/.minikube/machines/minikube/id_rsa Username:docker}
I0513 05:55:11.738656   27816 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0513 05:55:11.738667   27816 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0513 05:55:11.738750   27816 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 05:55:11.782991   27816 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61537 SSHKeyPath:/Users/gopher/.minikube/machines/minikube/id_rsa Username:docker}
I0513 05:55:11.813161   27816 start.go:946] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0513 05:55:11.813272   27816 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0513 05:55:11.852753   27816 api_server.go:52] waiting for apiserver process to appear ...
I0513 05:55:11.852798   27816 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0513 05:55:11.859227   27816 api_server.go:72] duration metric: took 245.119042ms to wait for apiserver process to appear ...
I0513 05:55:11.859235   27816 api_server.go:88] waiting for apiserver healthz status ...
I0513 05:55:11.859244   27816 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61541/healthz ...
I0513 05:55:11.862986   27816 api_server.go:279] https://127.0.0.1:61541/healthz returned 200:
ok
I0513 05:55:11.863805   27816 api_server.go:141] control plane version: v1.30.0
I0513 05:55:11.863812   27816 api_server.go:131] duration metric: took 4.5755ms to wait for apiserver health ...
I0513 05:55:11.864480   27816 system_pods.go:43] waiting for kube-system pods to appear ...
I0513 05:55:11.869225   27816 system_pods.go:59] 4 kube-system pods found
I0513 05:55:11.869238   27816 system_pods.go:61] "etcd-minikube" [a83012df-5a4c-408a-979b-242f6493954e] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0513 05:55:11.869247   27816 system_pods.go:61] "kube-apiserver-minikube" [9f0d5651-a949-4335-a4a8-101d25613fe1] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0513 05:55:11.869252   27816 system_pods.go:61] "kube-controller-manager-minikube" [04c86ebf-dd39-4d69-aefe-654d526eef8b] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0513 05:55:11.869254   27816 system_pods.go:61] "kube-scheduler-minikube" [26d21380-2a34-4020-92a3-d92ac4438e04] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0513 05:55:11.869261   27816 system_pods.go:74] duration metric: took 4.77775ms to wait for pod list to return data ...
I0513 05:55:11.869265   27816 kubeadm.go:576] duration metric: took 255.15975ms to wait for: map[apiserver:true system_pods:true]
I0513 05:55:11.869271   27816 node_conditions.go:102] verifying NodePressure condition ...
I0513 05:55:11.871533   27816 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0513 05:55:11.871625   27816 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0513 05:55:11.871630   27816 node_conditions.go:123] node cpu capacity is 12
I0513 05:55:11.871636   27816 node_conditions.go:105] duration metric: took 2.363458ms to run NodePressure ...
I0513 05:55:11.871641   27816 start.go:240] waiting for startup goroutines ...
I0513 05:55:11.872904   27816 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0513 05:55:12.009182   27816 out.go:177] üåü  Ïï†ÎìúÏò® ÌôúÏÑ±Ìôî : storage-provisioner, default-storageclass
I0513 05:55:12.014296   27816 addons.go:505] duration metric: took 400.221833ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0513 05:55:12.319394   27816 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0513 05:55:12.319422   27816 start.go:245] waiting for cluster config update ...
I0513 05:55:12.319446   27816 start.go:254] writing updated cluster config ...
I0513 05:55:12.320134   27816 ssh_runner.go:195] Run: rm -f paused
I0513 05:55:12.446507   27816 start.go:600] kubectl: 1.28.2, cluster: 1.30.0 (minor skew: 2)
I0513 05:55:12.449817   27816 out.go:177] 
W0513 05:55:12.452827   27816 out.go:239] ‚ùó  /usr/local/bin/kubectl is version 1.28.2, which may have incompatibilities with Kubernetes 1.30.0.
I0513 05:55:12.453914   27816 out.go:177]     ‚ñ™ Want kubectl v1.30.0? Try 'minikube kubectl -- get pods -A'
I0513 05:55:12.457774   27816 out.go:177] üèÑ  ÎÅùÎÇ¨ÏäµÎãàÎã§! kubectlÏù¥ "minikube" ÌÅ¥Îü¨Ïä§ÌÑ∞ÏôÄ "default" ÎÑ§ÏûÑÏä§ÌéòÏù¥Ïä§Î•º Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÏÇ¨Ïö©ÌïòÎèÑÎ°ù Íµ¨ÏÑ±ÎêòÏóàÏäµÎãàÎã§.


==> Docker <==
May 13 12:55:02 minikube dockerd[873]: time="2024-05-13T12:55:02.005772968Z" level=info msg="Processing signal 'terminated'"
May 13 12:55:02 minikube dockerd[873]: time="2024-05-13T12:55:02.013468510Z" level=info msg="API listen on /var/run/docker.sock"
May 13 12:55:02 minikube dockerd[873]: time="2024-05-13T12:55:02.013475676Z" level=info msg="API listen on [::]:2376"
May 13 12:55:02 minikube dockerd[873]: time="2024-05-13T12:55:02.014002926Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
May 13 12:55:02 minikube dockerd[873]: time="2024-05-13T12:55:02.014193676Z" level=info msg="Daemon shutdown complete"
May 13 12:55:02 minikube systemd[1]: docker.service: Deactivated successfully.
May 13 12:55:02 minikube systemd[1]: Stopped Docker Application Container Engine.
May 13 12:55:02 minikube systemd[1]: Starting Docker Application Container Engine...
May 13 12:55:02 minikube dockerd[1098]: time="2024-05-13T12:55:02.045654135Z" level=info msg="Starting up"
May 13 12:55:02 minikube dockerd[1098]: time="2024-05-13T12:55:02.053601010Z" level=info msg="[graphdriver] trying configured driver: overlay2"
May 13 12:55:02 minikube dockerd[1098]: time="2024-05-13T12:55:02.092681010Z" level=info msg="Loading containers: start."
May 13 12:55:02 minikube dockerd[1098]: time="2024-05-13T12:55:02.134241343Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
May 13 12:55:02 minikube dockerd[1098]: time="2024-05-13T12:55:02.148388551Z" level=info msg="Loading containers: done."
May 13 12:55:02 minikube dockerd[1098]: time="2024-05-13T12:55:02.176786760Z" level=info msg="Docker daemon" commit=60b9add containerd-snapshotter=false storage-driver=overlay2 version=26.0.1
May 13 12:55:02 minikube dockerd[1098]: time="2024-05-13T12:55:02.176845260Z" level=info msg="Daemon has completed initialization"
May 13 12:55:02 minikube dockerd[1098]: time="2024-05-13T12:55:02.198918343Z" level=info msg="API listen on /var/run/docker.sock"
May 13 12:55:02 minikube dockerd[1098]: time="2024-05-13T12:55:02.198926468Z" level=info msg="API listen on [::]:2376"
May 13 12:55:02 minikube systemd[1]: Started Docker Application Container Engine.
May 13 12:55:02 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
May 13 12:55:02 minikube cri-dockerd[1326]: time="2024-05-13T12:55:02Z" level=info msg="Starting cri-dockerd dev (HEAD)"
May 13 12:55:02 minikube cri-dockerd[1326]: time="2024-05-13T12:55:02Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
May 13 12:55:02 minikube cri-dockerd[1326]: time="2024-05-13T12:55:02Z" level=info msg="Start docker client with request timeout 0s"
May 13 12:55:02 minikube cri-dockerd[1326]: time="2024-05-13T12:55:02Z" level=info msg="Hairpin mode is set to hairpin-veth"
May 13 12:55:02 minikube cri-dockerd[1326]: time="2024-05-13T12:55:02Z" level=info msg="Loaded network plugin cni"
May 13 12:55:02 minikube cri-dockerd[1326]: time="2024-05-13T12:55:02Z" level=info msg="Docker cri networking managed by network plugin cni"
May 13 12:55:02 minikube cri-dockerd[1326]: time="2024-05-13T12:55:02Z" level=info msg="Setting cgroupDriver cgroupfs"
May 13 12:55:02 minikube cri-dockerd[1326]: time="2024-05-13T12:55:02Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
May 13 12:55:02 minikube cri-dockerd[1326]: time="2024-05-13T12:55:02Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
May 13 12:55:02 minikube cri-dockerd[1326]: time="2024-05-13T12:55:02Z" level=info msg="Start cri-dockerd grpc backend"
May 13 12:55:02 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
May 13 12:55:06 minikube cri-dockerd[1326]: time="2024-05-13T12:55:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/63af818ff2dbd7d341fc35254d3bb14574864a4df99eacc1561e90dfd1d5e6c9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 13 12:55:06 minikube cri-dockerd[1326]: time="2024-05-13T12:55:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/391d6004715689a013c8ee0f9e947d896edf647601294bf82cac6c36bac3300e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 13 12:55:06 minikube cri-dockerd[1326]: time="2024-05-13T12:55:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1a80b55fbb33ed5d7a1bfc4196455bee444caf77a2f935bd5178a85db127b6bb/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 13 12:55:06 minikube cri-dockerd[1326]: time="2024-05-13T12:55:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d236e6bf88eeb89a8377006a93794c89ea31ef235cce3b4697838ec3b7a04381/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 13 12:55:25 minikube cri-dockerd[1326]: time="2024-05-13T12:55:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e3f47784520ea28e8567ceb0d0ea265c2e1849bbac139ed79d7afa09252a0f9e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 13 12:55:25 minikube cri-dockerd[1326]: time="2024-05-13T12:55:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8405cd725de27544300df2831d57613c69a43da97f87e2937d5e1674350e5035/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 13 12:55:25 minikube cri-dockerd[1326]: time="2024-05-13T12:55:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1dcb8aa201923a98f51d6335680ac26606b31d83fb111530f67bdf9815cf0f5c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 13 12:55:25 minikube cri-dockerd[1326]: time="2024-05-13T12:55:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0978b3202b91d5f9269535da6bea30ea3221522c4ab16057b1430c502eaae4a7/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 13 12:55:30 minikube cri-dockerd[1326]: time="2024-05-13T12:55:30Z" level=info msg="Stop pulling image kicbase/echo-server:1.0: Status: Downloaded newer image for kicbase/echo-server:1.0"
May 13 12:55:31 minikube cri-dockerd[1326]: time="2024-05-13T12:55:31Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
May 13 13:13:43 minikube cri-dockerd[1326]: time="2024-05-13T13:13:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/491b51065c7936a82197fe3f69e68f0061f0275876e9db988f45161147bc2188/resolv.conf as [nameserver 10.96.0.10 search fast-api-example.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 13 13:13:43 minikube cri-dockerd[1326]: time="2024-05-13T13:13:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ea2ae974770bc0aa6eea77805f0ff3d18bb6e6ae07f4e71eca10e0001feab7fb/resolv.conf as [nameserver 10.96.0.10 search fast-api-example.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 13 13:13:45 minikube dockerd[1098]: time="2024-05-13T13:13:45.294206668Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=e412ceb00060127b traceID=b1e8c4ed4af74d6d24b0cef00c5ccd0c
May 13 13:13:45 minikube dockerd[1098]: time="2024-05-13T13:13:45.294390960Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 13 13:13:46 minikube dockerd[1098]: time="2024-05-13T13:13:46.630184419Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=7dde0ac48c725e69 traceID=60edc2780536cd541992f61627cf50ca
May 13 13:13:46 minikube dockerd[1098]: time="2024-05-13T13:13:46.630291544Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 13 13:13:58 minikube dockerd[1098]: time="2024-05-13T13:13:58.994374550Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=0e6c4e3a47279f02 traceID=d4e4f57cb59ef051efa338f78d62832a
May 13 13:13:58 minikube dockerd[1098]: time="2024-05-13T13:13:58.994538216Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 13 13:14:02 minikube dockerd[1098]: time="2024-05-13T13:14:02.016031676Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=217b86a9e0dd884f traceID=08d842ee704b410b590c62934c5599f7
May 13 13:14:02 minikube dockerd[1098]: time="2024-05-13T13:14:02.016167218Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 13 13:14:23 minikube dockerd[1098]: time="2024-05-13T13:14:23.097738880Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=27c9af9bed770e85 traceID=23fafe230669cb59bffde13076c2179c
May 13 13:14:23 minikube dockerd[1098]: time="2024-05-13T13:14:23.097808714Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 13 13:14:29 minikube dockerd[1098]: time="2024-05-13T13:14:29.976400509Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=2ac88ba57702c98d traceID=4ae7218f5c2eddd8e4ad51ba57e3249a
May 13 13:14:29 minikube dockerd[1098]: time="2024-05-13T13:14:29.976465634Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 13 13:15:06 minikube dockerd[1098]: time="2024-05-13T13:15:06.136931178Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=5b3a5a048ffc1987 traceID=c18b4127098631a1ed1a262ac22d5db8
May 13 13:15:06 minikube dockerd[1098]: time="2024-05-13T13:15:06.137061011Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 13 13:15:22 minikube dockerd[1098]: time="2024-05-13T13:15:22.071890588Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=29abb6d4616130c2 traceID=5a65d9f4da7036da89fcf63ec26da1b0
May 13 13:15:22 minikube dockerd[1098]: time="2024-05-13T13:15:22.076010921Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 13 13:16:30 minikube dockerd[1098]: time="2024-05-13T13:16:30.066619842Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=b76405863efb7eeb traceID=e8dd29b11b4cc3951dd6e1f84fe4afdc
May 13 13:16:30 minikube dockerd[1098]: time="2024-05-13T13:16:30.066668509Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                                         CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
d365f62e965c3       kicbase/echo-server@sha256:127ac38a2bb9537b7f252addff209ea6801edcac8a92c8b1104dacd66a583ed6   21 minutes ago      Running             echo-server               0                   0978b3202b91d       hello-minikube-5c898d8489-fzrb8
7c6360ab32fc4       2437cf7621777                                                                                 21 minutes ago      Running             coredns                   0                   1dcb8aa201923       coredns-7db6d8ff4d-k85fh
0c57ebb9de41e       ba04bb24b9575                                                                                 21 minutes ago      Running             storage-provisioner       0                   8405cd725de27       storage-provisioner
c76fd38726fa0       cb7eac0b42cc1                                                                                 21 minutes ago      Running             kube-proxy                0                   e3f47784520ea       kube-proxy-gzg4n
73ca16f1fd21c       181f57fd3cdb7                                                                                 21 minutes ago      Running             kube-apiserver            0                   d236e6bf88eeb       kube-apiserver-minikube
7a7357b9651c3       014faa467e297                                                                                 21 minutes ago      Running             etcd                      0                   63af818ff2dbd       etcd-minikube
2f455bbc35413       68feac521c0f1                                                                                 21 minutes ago      Running             kube-controller-manager   0                   1a80b55fbb33e       kube-controller-manager-minikube
c5deca0b7eee0       547adae34140b                                                                                 21 minutes ago      Running             kube-scheduler            0                   391d600471568       kube-scheduler-minikube


==> coredns [7c6360ab32fc] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/arm64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:50195 - 36489 "HINFO IN 5160281731438346704.2443239153059614762. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.121659875s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_05_13T05_55_11_0700
                    minikube.k8s.io/version=v1.33.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 13 May 2024 12:55:08 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 13 May 2024 13:16:37 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 13 May 2024 13:16:06 +0000   Mon, 13 May 2024 12:55:08 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 13 May 2024 13:16:06 +0000   Mon, 13 May 2024 12:55:08 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 13 May 2024 13:16:06 +0000   Mon, 13 May 2024 12:55:08 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 13 May 2024 13:16:06 +0000   Mon, 13 May 2024 12:55:09 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.67.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  61202244Ki
  memory:             8034500Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  61202244Ki
  memory:             8034500Ki
  pods:               110
System Info:
  Machine ID:                 ee626b5fac5a40e9b77e77ec80f531a8
  System UUID:                ee626b5fac5a40e9b77e77ec80f531a8
  Boot ID:                    a67a4308-9c9b-4884-aa2f-118e0a96b075
  Kernel Version:             6.5.11-linuxkit
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://26.0.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  default                     hello-minikube-5c898d8489-fzrb8         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21m
  fast-api-example            fast-api-deployment-6c558f65dd-8h5jj    500m (4%!)(MISSING)     500m (4%!)(MISSING)   256Mi (3%!)(MISSING)       256Mi (3%!)(MISSING)     2m57s
  fast-api-example            fast-api-deployment-6c558f65dd-cmfn2    500m (4%!)(MISSING)     500m (4%!)(MISSING)   256Mi (3%!)(MISSING)       256Mi (3%!)(MISSING)     2m57s
  kube-system                 coredns-7db6d8ff4d-k85fh                100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     21m
  kube-system                 etcd-minikube                           100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         21m
  kube-system                 kube-apiserver-minikube                 250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21m
  kube-system                 kube-controller-manager-minikube        200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21m
  kube-system                 kube-proxy-gzg4n                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21m
  kube-system                 kube-scheduler-minikube                 100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21m
  kube-system                 storage-provisioner                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1750m (14%!)(MISSING)  1 (8%!)(MISSING)
  memory             682Mi (8%!)(MISSING)   682Mi (8%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:
  Type    Reason                   Age   From             Message
  ----    ------                   ----  ----             -------
  Normal  Starting                 21m   kube-proxy       
  Normal  Starting                 21m   kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  21m   kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  21m   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    21m   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     21m   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           21m   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[May13 12:54] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.081141] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.094692] netlink: 'init': attribute type 4 has an invalid length.
[  +0.018812] netlink: 'init': attribute type 22 has an invalid length.
[  +0.032845] grpcfuse: loading out-of-tree module taints kernel.
[May13 12:55] systemd[2068]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[May13 12:56] hrtimer: interrupt took 79910250 ns


==> etcd [7a7357b9651c] <==
{"level":"info","ts":"2024-05-13T12:55:07.196532Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"8688e899f7831fc7","cluster-id":"9d8fdeb88b6def78"}
{"level":"info","ts":"2024-05-13T12:55:07.196591Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 switched to configuration voters=()"}
{"level":"info","ts":"2024-05-13T12:55:07.196624Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 became follower at term 0"}
{"level":"info","ts":"2024-05-13T12:55:07.196628Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft 8688e899f7831fc7 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-05-13T12:55:07.196635Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 became follower at term 1"}
{"level":"info","ts":"2024-05-13T12:55:07.196657Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 switched to configuration voters=(9694253945895198663)"}
{"level":"warn","ts":"2024-05-13T12:55:07.199913Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-05-13T12:55:07.201767Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-05-13T12:55:07.203027Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-05-13T12:55:07.206232Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"8688e899f7831fc7","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-05-13T12:55:07.206682Z","caller":"etcdserver/server.go:744","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"8688e899f7831fc7","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-05-13T12:55:07.206731Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-13T12:55:07.206761Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-13T12:55:07.206765Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-13T12:55:07.20764Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 switched to configuration voters=(9694253945895198663)"}
{"level":"info","ts":"2024-05-13T12:55:07.207848Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"9d8fdeb88b6def78","local-member-id":"8688e899f7831fc7","added-peer-id":"8688e899f7831fc7","added-peer-peer-urls":["https://192.168.67.2:2380"]}
{"level":"info","ts":"2024-05-13T12:55:07.208133Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-05-13T12:55:07.208181Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.67.2:2380"}
{"level":"info","ts":"2024-05-13T12:55:07.208194Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.67.2:2380"}
{"level":"info","ts":"2024-05-13T12:55:07.208266Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"8688e899f7831fc7","initial-advertise-peer-urls":["https://192.168.67.2:2380"],"listen-peer-urls":["https://192.168.67.2:2380"],"advertise-client-urls":["https://192.168.67.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.67.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-05-13T12:55:07.208281Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-05-13T12:55:07.49699Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 is starting a new election at term 1"}
{"level":"info","ts":"2024-05-13T12:55:07.497032Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 became pre-candidate at term 1"}
{"level":"info","ts":"2024-05-13T12:55:07.497283Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 received MsgPreVoteResp from 8688e899f7831fc7 at term 1"}
{"level":"info","ts":"2024-05-13T12:55:07.497327Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 became candidate at term 2"}
{"level":"info","ts":"2024-05-13T12:55:07.49734Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 received MsgVoteResp from 8688e899f7831fc7 at term 2"}
{"level":"info","ts":"2024-05-13T12:55:07.497345Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 became leader at term 2"}
{"level":"info","ts":"2024-05-13T12:55:07.497349Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 8688e899f7831fc7 elected leader 8688e899f7831fc7 at term 2"}
{"level":"info","ts":"2024-05-13T12:55:07.498129Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"8688e899f7831fc7","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.67.2:2379]}","request-path":"/0/members/8688e899f7831fc7/attributes","cluster-id":"9d8fdeb88b6def78","publish-timeout":"7s"}
{"level":"info","ts":"2024-05-13T12:55:07.498141Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-13T12:55:07.498207Z","caller":"etcdserver/server.go:2578","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-13T12:55:07.498214Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-13T12:55:07.498462Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-05-13T12:55:07.498481Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-05-13T12:55:07.498931Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"9d8fdeb88b6def78","local-member-id":"8688e899f7831fc7","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-13T12:55:07.499043Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-13T12:55:07.499056Z","caller":"etcdserver/server.go:2602","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-13T12:55:07.499333Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.67.2:2379"}
{"level":"info","ts":"2024-05-13T12:55:07.499333Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-05-13T13:05:07.632192Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":630}
{"level":"info","ts":"2024-05-13T13:05:07.637677Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":630,"took":"5.062875ms","hash":93044146,"current-db-size-bytes":1384448,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1384448,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-05-13T13:05:07.637891Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":93044146,"revision":630,"compact-revision":-1}
{"level":"info","ts":"2024-05-13T13:05:37.890925Z","caller":"traceutil/trace.go:171","msg":"trace[1418235058] transaction","detail":"{read_only:false; response_revision:893; number_of_response:1; }","duration":"170.753458ms","start":"2024-05-13T13:05:37.720159Z","end":"2024-05-13T13:05:37.890913Z","steps":["trace[1418235058] 'process raft request'  (duration: 170.657167ms)"],"step_count":1}
{"level":"info","ts":"2024-05-13T13:08:09.326507Z","caller":"traceutil/trace.go:171","msg":"trace[1436999347] transaction","detail":"{read_only:false; response_revision:1014; number_of_response:1; }","duration":"241.36475ms","start":"2024-05-13T13:08:09.085117Z","end":"2024-05-13T13:08:09.326482Z","steps":["trace[1436999347] 'process raft request'  (duration: 241.038542ms)"],"step_count":1}
{"level":"info","ts":"2024-05-13T13:08:11.504312Z","caller":"traceutil/trace.go:171","msg":"trace[1579345533] transaction","detail":"{read_only:false; response_revision:1016; number_of_response:1; }","duration":"172.348167ms","start":"2024-05-13T13:08:11.331949Z","end":"2024-05-13T13:08:11.504297Z","steps":["trace[1579345533] 'process raft request'  (duration: 172.265042ms)"],"step_count":1}
{"level":"info","ts":"2024-05-13T13:08:22.200499Z","caller":"traceutil/trace.go:171","msg":"trace[1432796438] linearizableReadLoop","detail":"{readStateIndex:1191; appliedIndex:1190; }","duration":"350.725251ms","start":"2024-05-13T13:08:21.849755Z","end":"2024-05-13T13:08:22.20048Z","steps":["trace[1432796438] 'read index received'  (duration: 350.555334ms)","trace[1432796438] 'applied index is now lower than readState.Index'  (duration: 169.417¬µs)"],"step_count":2}
{"level":"info","ts":"2024-05-13T13:08:22.200617Z","caller":"traceutil/trace.go:171","msg":"trace[1732521846] transaction","detail":"{read_only:false; response_revision:1024; number_of_response:1; }","duration":"660.661875ms","start":"2024-05-13T13:08:21.539947Z","end":"2024-05-13T13:08:22.200608Z","steps":["trace[1732521846] 'process raft request'  (duration: 660.373417ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-13T13:08:22.200773Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"315.784209ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csinodes/\" range_end:\"/registry/csinodes0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"warn","ts":"2024-05-13T13:08:22.200845Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"350.856126ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-05-13T13:08:22.200854Z","caller":"traceutil/trace.go:171","msg":"trace[1195344683] range","detail":"{range_begin:/registry/csinodes/; range_end:/registry/csinodes0; response_count:0; response_revision:1024; }","duration":"315.992917ms","start":"2024-05-13T13:08:21.884821Z","end":"2024-05-13T13:08:22.200814Z","steps":["trace[1195344683] 'agreement among raft nodes before linearized reading'  (duration: 315.750417ms)"],"step_count":1}
{"level":"info","ts":"2024-05-13T13:08:22.200866Z","caller":"traceutil/trace.go:171","msg":"trace[579236953] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1024; }","duration":"351.109584ms","start":"2024-05-13T13:08:21.849749Z","end":"2024-05-13T13:08:22.200859Z","steps":["trace[579236953] 'agreement among raft nodes before linearized reading'  (duration: 350.822542ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-13T13:08:22.200882Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-13T13:08:21.849718Z","time spent":"351.15775ms","remote":"127.0.0.1:53380","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-05-13T13:08:22.200876Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-13T13:08:21.884796Z","time spent":"316.072501ms","remote":"127.0.0.1:53756","response type":"/etcdserverpb.KV/Range","request count":0,"request size":44,"response count":1,"response size":31,"request content":"key:\"/registry/csinodes/\" range_end:\"/registry/csinodes0\" count_only:true "}
{"level":"warn","ts":"2024-05-13T13:08:22.201315Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-13T13:08:21.539937Z","time spent":"660.717417ms","remote":"127.0.0.1:53518","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1022 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-05-13T13:10:07.644546Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":869}
{"level":"info","ts":"2024-05-13T13:10:07.649054Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":869,"took":"3.469333ms","hash":932293702,"current-db-size-bytes":1384448,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":897024,"current-db-size-in-use":"897 kB"}
{"level":"info","ts":"2024-05-13T13:10:07.649169Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":932293702,"revision":869,"compact-revision":630}
{"level":"info","ts":"2024-05-13T13:15:07.65163Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1107}
{"level":"info","ts":"2024-05-13T13:15:07.654028Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1107,"took":"1.787875ms","hash":1308509521,"current-db-size-bytes":1384448,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1175552,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2024-05-13T13:15:07.654073Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1308509521,"revision":1107,"compact-revision":869}


==> kernel <==
 13:16:40 up 22 min,  0 users,  load average: 0.16, 0.19, 0.18
Linux minikube 6.5.11-linuxkit #1 SMP PREEMPT Wed Dec  6 17:08:31 UTC 2023 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [73ca16f1fd21] <==
I0513 12:55:08.613174       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0513 12:55:08.613231       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0513 12:55:08.613326       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0513 12:55:08.613336       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0513 12:55:08.613415       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0513 12:55:08.613447       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0513 12:55:08.613451       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0513 12:55:08.613489       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0513 12:55:08.613542       1 controller.go:78] Starting OpenAPI AggregationController
I0513 12:55:08.613546       1 controller.go:116] Starting legacy_token_tracking_controller
I0513 12:55:08.613555       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0513 12:55:08.613560       1 available_controller.go:423] Starting AvailableConditionController
I0513 12:55:08.613575       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0513 12:55:08.613586       1 controller.go:139] Starting OpenAPI controller
I0513 12:55:08.613554       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0513 12:55:08.613649       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0513 12:55:08.613701       1 controller.go:87] Starting OpenAPI V3 controller
I0513 12:55:08.613739       1 naming_controller.go:291] Starting NamingConditionController
I0513 12:55:08.613763       1 establishing_controller.go:76] Starting EstablishingController
I0513 12:55:08.613781       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0513 12:55:08.613793       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0513 12:55:08.613805       1 crd_finalizer.go:266] Starting CRDFinalizer
I0513 12:55:08.699080       1 shared_informer.go:320] Caches are synced for node_authorizer
I0513 12:55:08.701432       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0513 12:55:08.701453       1 policy_source.go:224] refreshing policies
I0513 12:55:08.713469       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0513 12:55:08.713573       1 aggregator.go:165] initial CRD sync complete...
I0513 12:55:08.713587       1 autoregister_controller.go:141] Starting autoregister controller
I0513 12:55:08.713592       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0513 12:55:08.713595       1 cache.go:39] Caches are synced for autoregister controller
I0513 12:55:08.713716       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0513 12:55:08.713740       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0513 12:55:08.713756       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0513 12:55:08.713770       1 shared_informer.go:320] Caches are synced for configmaps
I0513 12:55:08.713775       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0513 12:55:08.713813       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0513 12:55:08.713866       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0513 12:55:08.714819       1 controller.go:615] quota admission added evaluator for: namespaces
I0513 12:55:08.777388       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0513 12:55:09.615799       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0513 12:55:09.617356       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0513 12:55:09.617377       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0513 12:55:09.780995       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0513 12:55:09.791893       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0513 12:55:09.822881       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0513 12:55:09.825090       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.67.2]
I0513 12:55:09.825554       1 controller.go:615] quota admission added evaluator for: endpoints
I0513 12:55:09.827819       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0513 12:55:10.683221       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0513 12:55:10.929693       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0513 12:55:10.933316       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0513 12:55:10.936414       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0513 12:55:12.567496       1 alloc.go:330] "allocated clusterIPs" service="default/hello-minikube" clusterIPs={"IPv4":"10.96.248.251"}
I0513 12:55:24.724591       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0513 12:55:24.773853       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0513 13:08:22.202157       1 trace.go:236] Trace[172532038]: "Update" accept:application/json, */*,audit-id:ec6b0aa7-cae3-47cd-af0f-adb74e0f736d,client:192.168.67.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/arm64) kubernetes/$Format,verb:PUT (13-May-2024 13:08:21.539) (total time: 662ms):
Trace[172532038]: ["GuaranteedUpdate etcd3" audit-id:ec6b0aa7-cae3-47cd-af0f-adb74e0f736d,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 662ms (13:08:21.539)
Trace[172532038]:  ---"Txn call completed" 661ms (13:08:22.201)]
Trace[172532038]: [662.992459ms] [662.992459ms] END
I0513 13:13:43.508178       1 alloc.go:330] "allocated clusterIPs" service="fast-api-example/fast-api-service" clusterIPs={"IPv4":"10.105.186.211"}


==> kube-controller-manager [2f455bbc3541] <==
I0513 12:55:24.131330       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0513 12:55:24.131336       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0513 12:55:24.131343       1 shared_informer.go:320] Caches are synced for cidrallocator
I0513 12:55:24.131438       1 shared_informer.go:320] Caches are synced for PV protection
I0513 12:55:24.134977       1 range_allocator.go:381] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0513 12:55:24.138352       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0513 12:55:24.150370       1 shared_informer.go:320] Caches are synced for attach detach
I0513 12:55:24.164669       1 shared_informer.go:320] Caches are synced for taint
I0513 12:55:24.164771       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0513 12:55:24.164825       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0513 12:55:24.164849       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0513 12:55:24.167970       1 shared_informer.go:320] Caches are synced for persistent volume
I0513 12:55:24.174707       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0513 12:55:24.207640       1 shared_informer.go:320] Caches are synced for disruption
I0513 12:55:24.251369       1 shared_informer.go:320] Caches are synced for resource quota
I0513 12:55:24.272653       1 shared_informer.go:320] Caches are synced for TTL after finished
I0513 12:55:24.283991       1 shared_informer.go:320] Caches are synced for job
I0513 12:55:24.312667       1 shared_informer.go:320] Caches are synced for cronjob
I0513 12:55:24.325029       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0513 12:55:24.325311       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0513 12:55:24.326910       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0513 12:55:24.326957       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0513 12:55:24.330625       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0513 12:55:24.337327       1 shared_informer.go:320] Caches are synced for resource quota
I0513 12:55:24.759368       1 shared_informer.go:320] Caches are synced for garbage collector
I0513 12:55:24.826226       1 shared_informer.go:320] Caches are synced for garbage collector
I0513 12:55:24.826245       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0513 12:55:25.228503       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="501.131833ms"
I0513 12:55:25.228560       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-5c898d8489" duration="501.968ms"
I0513 12:55:25.230120       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-5c898d8489" duration="1.540333ms"
I0513 12:55:25.230154       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-5c898d8489" duration="13.584¬µs"
I0513 12:55:25.231168       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-5c898d8489" duration="31.834¬µs"
I0513 12:55:25.232910       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="4.382083ms"
I0513 12:55:25.232947       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="22¬µs"
I0513 12:55:25.236564       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="31.125¬µs"
I0513 12:55:25.774739       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="34.083¬µs"
I0513 12:55:26.807373       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="4.605458ms"
I0513 12:55:26.807437       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="33.625¬µs"
I0513 12:55:30.831301       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-5c898d8489" duration="4.431584ms"
I0513 12:55:30.831426       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-5c898d8489" duration="41.875¬µs"
I0513 13:13:43.425391       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="9.202208ms"
I0513 13:13:43.430885       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="5.435625ms"
I0513 13:13:43.430937       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="24.625¬µs"
I0513 13:13:43.433050       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="20.417¬µs"
I0513 13:13:46.338788       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="57.125¬µs"
I0513 13:13:47.346260       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="132.667¬µs"
I0513 13:13:57.707380       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="427.791¬µs"
I0513 13:14:00.694908       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="64.5¬µs"
I0513 13:14:09.698984       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="226.917¬µs"
I0513 13:14:15.699476       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="91.5¬µs"
I0513 13:14:21.699487       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="84.125¬µs"
I0513 13:14:28.696118       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="77.791¬µs"
I0513 13:14:36.703378       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="238.5¬µs"
I0513 13:14:42.689062       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="54.5¬µs"
I0513 13:14:51.697842       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="136.25¬µs"
I0513 13:14:57.704830       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="130.209¬µs"
I0513 13:15:20.704429       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="106.791¬µs"
I0513 13:15:35.699157       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="184.958¬µs"
I0513 13:15:37.703895       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="119.667¬µs"
I0513 13:15:51.698541       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fast-api-example/fast-api-deployment-6c558f65dd" duration="252.542¬µs"


==> kube-proxy [c76fd38726fa] <==
I0513 12:55:25.283739       1 server_linux.go:69] "Using iptables proxy"
I0513 12:55:25.291338       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.67.2"]
I0513 12:55:25.305921       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0513 12:55:25.305957       1 server_linux.go:165] "Using iptables Proxier"
I0513 12:55:25.306992       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0513 12:55:25.307004       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0513 12:55:25.307043       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0513 12:55:25.307294       1 server.go:872] "Version info" version="v1.30.0"
I0513 12:55:25.307310       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0513 12:55:25.308346       1 config.go:192] "Starting service config controller"
I0513 12:55:25.308422       1 shared_informer.go:313] Waiting for caches to sync for service config
I0513 12:55:25.308670       1 config.go:101] "Starting endpoint slice config controller"
I0513 12:55:25.308679       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0513 12:55:25.308686       1 config.go:319] "Starting node config controller"
I0513 12:55:25.308691       1 shared_informer.go:313] Waiting for caches to sync for node config
I0513 12:55:25.408838       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0513 12:55:25.408841       1 shared_informer.go:320] Caches are synced for node config
I0513 12:55:25.408854       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [c5deca0b7eee] <==
I0513 12:55:07.401978       1 serving.go:380] Generated self-signed cert in-memory
W0513 12:55:08.679677       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0513 12:55:08.679748       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0513 12:55:08.679763       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0513 12:55:08.679767       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0513 12:55:08.689765       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0513 12:55:08.689794       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0513 12:55:08.691079       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0513 12:55:08.691111       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0513 12:55:08.691109       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0513 12:55:08.691137       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W0513 12:55:08.692222       1 reflector.go:547] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0513 12:55:08.692294       1 reflector.go:150] runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0513 12:55:08.692507       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0513 12:55:08.692529       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0513 12:55:08.692672       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0513 12:55:08.692706       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0513 12:55:08.692847       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0513 12:55:08.692876       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0513 12:55:08.692682       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0513 12:55:08.692895       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0513 12:55:08.692936       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0513 12:55:08.692955       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0513 12:55:08.693000       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0513 12:55:08.693011       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0513 12:55:08.693014       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0513 12:55:08.693026       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0513 12:55:08.693059       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0513 12:55:08.693068       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0513 12:55:08.693417       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0513 12:55:08.693435       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0513 12:55:08.693470       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0513 12:55:08.693488       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0513 12:55:08.693507       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0513 12:55:08.693516       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0513 12:55:08.693598       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0513 12:55:08.693605       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0513 12:55:08.693611       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0513 12:55:08.693615       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0513 12:55:08.693648       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0513 12:55:08.693657       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0513 12:55:09.518631       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0513 12:55:09.518704       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0513 12:55:09.545724       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0513 12:55:09.545747       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0513 12:55:09.650728       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0513 12:55:09.650755       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0513 12:55:09.664319       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0513 12:55:09.664337       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0513 12:55:09.698726       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0513 12:55:09.698757       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0513 12:55:09.703235       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0513 12:55:09.703255       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0513 12:55:09.705728       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0513 12:55:09.705749       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0513 12:55:09.710234       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0513 12:55:09.710250       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
I0513 12:55:10.191826       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
May 13 12:55:31 minikube kubelet[2233]: I0513 12:55:31.087113    2233 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
May 13 13:13:43 minikube kubelet[2233]: I0513 13:13:43.424821    2233 topology_manager.go:215] "Topology Admit Handler" podUID="b73fc512-ded2-4386-9d27-305f87b4e0da" podNamespace="fast-api-example" podName="fast-api-deployment-6c558f65dd-cmfn2"
May 13 13:13:43 minikube kubelet[2233]: I0513 13:13:43.425060    2233 topology_manager.go:215] "Topology Admit Handler" podUID="8c3334b8-2a1a-409c-8e37-e42ae448d38d" podNamespace="fast-api-example" podName="fast-api-deployment-6c558f65dd-8h5jj"
May 13 13:13:43 minikube kubelet[2233]: I0513 13:13:43.479573    2233 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-k9qht\" (UniqueName: \"kubernetes.io/projected/8c3334b8-2a1a-409c-8e37-e42ae448d38d-kube-api-access-k9qht\") pod \"fast-api-deployment-6c558f65dd-8h5jj\" (UID: \"8c3334b8-2a1a-409c-8e37-e42ae448d38d\") " pod="fast-api-example/fast-api-deployment-6c558f65dd-8h5jj"
May 13 13:13:43 minikube kubelet[2233]: I0513 13:13:43.479607    2233 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-9dwbn\" (UniqueName: \"kubernetes.io/projected/b73fc512-ded2-4386-9d27-305f87b4e0da-kube-api-access-9dwbn\") pod \"fast-api-deployment-6c558f65dd-cmfn2\" (UID: \"b73fc512-ded2-4386-9d27-305f87b4e0da\") " pod="fast-api-example/fast-api-deployment-6c558f65dd-cmfn2"
May 13 13:13:45 minikube kubelet[2233]: E0513 13:13:45.299287    2233 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:13:45 minikube kubelet[2233]: E0513 13:13:45.299687    2233 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:13:45 minikube kubelet[2233]: E0513 13:13:45.300338    2233 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:fast-api-example:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k9qht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6c558f65dd-8h5jj_fast-api-example(8c3334b8-2a1a-409c-8e37-e42ae448d38d): ErrImagePull: Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 13 13:13:45 minikube kubelet[2233]: E0513 13:13:45.300447    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImagePull: \"Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-8h5jj" podUID="8c3334b8-2a1a-409c-8e37-e42ae448d38d"
May 13 13:13:46 minikube kubelet[2233]: E0513 13:13:46.324825    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-8h5jj" podUID="8c3334b8-2a1a-409c-8e37-e42ae448d38d"
May 13 13:13:46 minikube kubelet[2233]: E0513 13:13:46.635961    2233 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:13:46 minikube kubelet[2233]: E0513 13:13:46.636129    2233 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:13:46 minikube kubelet[2233]: E0513 13:13:46.636472    2233 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:fast-api-example:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9dwbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6c558f65dd-cmfn2_fast-api-example(b73fc512-ded2-4386-9d27-305f87b4e0da): ErrImagePull: Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 13 13:13:46 minikube kubelet[2233]: E0513 13:13:46.636568    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImagePull: \"Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-cmfn2" podUID="b73fc512-ded2-4386-9d27-305f87b4e0da"
May 13 13:13:47 minikube kubelet[2233]: E0513 13:13:47.332640    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-cmfn2" podUID="b73fc512-ded2-4386-9d27-305f87b4e0da"
May 13 13:13:59 minikube kubelet[2233]: E0513 13:13:58.999907    2233 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:13:59 minikube kubelet[2233]: E0513 13:13:59.000034    2233 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:13:59 minikube kubelet[2233]: E0513 13:13:59.001233    2233 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:fast-api-example:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k9qht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6c558f65dd-8h5jj_fast-api-example(8c3334b8-2a1a-409c-8e37-e42ae448d38d): ErrImagePull: Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 13 13:13:59 minikube kubelet[2233]: E0513 13:13:59.001297    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImagePull: \"Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-8h5jj" podUID="8c3334b8-2a1a-409c-8e37-e42ae448d38d"
May 13 13:14:02 minikube kubelet[2233]: E0513 13:14:02.022628    2233 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:14:02 minikube kubelet[2233]: E0513 13:14:02.022752    2233 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:14:02 minikube kubelet[2233]: E0513 13:14:02.027782    2233 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:fast-api-example:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9dwbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6c558f65dd-cmfn2_fast-api-example(b73fc512-ded2-4386-9d27-305f87b4e0da): ErrImagePull: Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 13 13:14:02 minikube kubelet[2233]: E0513 13:14:02.028109    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImagePull: \"Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-cmfn2" podUID="b73fc512-ded2-4386-9d27-305f87b4e0da"
May 13 13:14:09 minikube kubelet[2233]: E0513 13:14:09.685484    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-8h5jj" podUID="8c3334b8-2a1a-409c-8e37-e42ae448d38d"
May 13 13:14:15 minikube kubelet[2233]: E0513 13:14:15.686421    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-cmfn2" podUID="b73fc512-ded2-4386-9d27-305f87b4e0da"
May 13 13:14:23 minikube kubelet[2233]: E0513 13:14:23.103284    2233 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:14:23 minikube kubelet[2233]: E0513 13:14:23.103397    2233 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:14:23 minikube kubelet[2233]: E0513 13:14:23.103648    2233 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:fast-api-example:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k9qht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6c558f65dd-8h5jj_fast-api-example(8c3334b8-2a1a-409c-8e37-e42ae448d38d): ErrImagePull: Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 13 13:14:23 minikube kubelet[2233]: E0513 13:14:23.103739    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImagePull: \"Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-8h5jj" podUID="8c3334b8-2a1a-409c-8e37-e42ae448d38d"
May 13 13:14:29 minikube kubelet[2233]: E0513 13:14:29.981190    2233 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:14:29 minikube kubelet[2233]: E0513 13:14:29.981291    2233 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:14:29 minikube kubelet[2233]: E0513 13:14:29.981500    2233 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:fast-api-example:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9dwbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6c558f65dd-cmfn2_fast-api-example(b73fc512-ded2-4386-9d27-305f87b4e0da): ErrImagePull: Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 13 13:14:29 minikube kubelet[2233]: E0513 13:14:29.981577    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImagePull: \"Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-cmfn2" podUID="b73fc512-ded2-4386-9d27-305f87b4e0da"
May 13 13:14:36 minikube kubelet[2233]: E0513 13:14:36.684991    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-8h5jj" podUID="8c3334b8-2a1a-409c-8e37-e42ae448d38d"
May 13 13:14:42 minikube kubelet[2233]: E0513 13:14:42.681265    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-cmfn2" podUID="b73fc512-ded2-4386-9d27-305f87b4e0da"
May 13 13:14:51 minikube kubelet[2233]: E0513 13:14:51.684774    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-8h5jj" podUID="8c3334b8-2a1a-409c-8e37-e42ae448d38d"
May 13 13:14:57 minikube kubelet[2233]: E0513 13:14:57.688025    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-cmfn2" podUID="b73fc512-ded2-4386-9d27-305f87b4e0da"
May 13 13:15:06 minikube kubelet[2233]: E0513 13:15:06.144265    2233 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:15:06 minikube kubelet[2233]: E0513 13:15:06.144409    2233 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:15:06 minikube kubelet[2233]: E0513 13:15:06.144653    2233 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:fast-api-example:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k9qht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6c558f65dd-8h5jj_fast-api-example(8c3334b8-2a1a-409c-8e37-e42ae448d38d): ErrImagePull: Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 13 13:15:06 minikube kubelet[2233]: E0513 13:15:06.144744    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImagePull: \"Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-8h5jj" podUID="8c3334b8-2a1a-409c-8e37-e42ae448d38d"
May 13 13:15:08 minikube kubelet[2233]: E0513 13:15:08.685395    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-cmfn2" podUID="b73fc512-ded2-4386-9d27-305f87b4e0da"
May 13 13:15:20 minikube kubelet[2233]: E0513 13:15:20.693060    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-8h5jj" podUID="8c3334b8-2a1a-409c-8e37-e42ae448d38d"
May 13 13:15:22 minikube kubelet[2233]: E0513 13:15:22.083993    2233 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:15:22 minikube kubelet[2233]: E0513 13:15:22.084511    2233 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:15:22 minikube kubelet[2233]: E0513 13:15:22.085033    2233 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:fast-api-example:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9dwbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6c558f65dd-cmfn2_fast-api-example(b73fc512-ded2-4386-9d27-305f87b4e0da): ErrImagePull: Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 13 13:15:22 minikube kubelet[2233]: E0513 13:15:22.085103    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImagePull: \"Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-cmfn2" podUID="b73fc512-ded2-4386-9d27-305f87b4e0da"
May 13 13:15:35 minikube kubelet[2233]: E0513 13:15:35.684004    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-8h5jj" podUID="8c3334b8-2a1a-409c-8e37-e42ae448d38d"
May 13 13:15:37 minikube kubelet[2233]: E0513 13:15:37.686069    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-cmfn2" podUID="b73fc512-ded2-4386-9d27-305f87b4e0da"
May 13 13:15:50 minikube kubelet[2233]: E0513 13:15:50.687006    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-8h5jj" podUID="8c3334b8-2a1a-409c-8e37-e42ae448d38d"
May 13 13:15:51 minikube kubelet[2233]: E0513 13:15:51.686026    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-cmfn2" podUID="b73fc512-ded2-4386-9d27-305f87b4e0da"
May 13 13:16:02 minikube kubelet[2233]: E0513 13:16:02.689918    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-cmfn2" podUID="b73fc512-ded2-4386-9d27-305f87b4e0da"
May 13 13:16:05 minikube kubelet[2233]: E0513 13:16:05.687482    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-8h5jj" podUID="8c3334b8-2a1a-409c-8e37-e42ae448d38d"
May 13 13:16:16 minikube kubelet[2233]: E0513 13:16:16.682410    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-cmfn2" podUID="b73fc512-ded2-4386-9d27-305f87b4e0da"
May 13 13:16:17 minikube kubelet[2233]: E0513 13:16:17.683155    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-8h5jj" podUID="8c3334b8-2a1a-409c-8e37-e42ae448d38d"
May 13 13:16:30 minikube kubelet[2233]: E0513 13:16:30.069019    2233 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:16:30 minikube kubelet[2233]: E0513 13:16:30.069062    2233 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="fast-api-example:latest"
May 13 13:16:30 minikube kubelet[2233]: E0513 13:16:30.069131    2233 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:fast-api-example:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k9qht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6c558f65dd-8h5jj_fast-api-example(8c3334b8-2a1a-409c-8e37-e42ae448d38d): ErrImagePull: Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
May 13 13:16:30 minikube kubelet[2233]: E0513 13:16:30.069148    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImagePull: \"Error response from daemon: pull access denied for fast-api-example, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-8h5jj" podUID="8c3334b8-2a1a-409c-8e37-e42ae448d38d"
May 13 13:16:30 minikube kubelet[2233]: E0513 13:16:30.687016    2233 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ImagePullBackOff: \"Back-off pulling image \\\"fast-api-example:latest\\\"\"" pod="fast-api-example/fast-api-deployment-6c558f65dd-cmfn2" podUID="b73fc512-ded2-4386-9d27-305f87b4e0da"


==> storage-provisioner [0c57ebb9de41] <==
I0513 12:55:25.503063       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0513 12:55:25.507308       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0513 12:55:25.507378       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0513 12:55:25.509925       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0513 12:55:25.509975       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"ba0e3248-1632-43ce-a7f9-7ed7c338d1ab", APIVersion:"v1", ResourceVersion:"378", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_8e143071-b5d9-4dcc-83da-d77e029de72e became leader
I0513 12:55:25.510012       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_8e143071-b5d9-4dcc-83da-d77e029de72e!
I0513 12:55:25.610637       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_8e143071-b5d9-4dcc-83da-d77e029de72e!

